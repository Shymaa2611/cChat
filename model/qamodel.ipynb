{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-18T02:33:00.398972Z","iopub.status.busy":"2024-10-18T02:33:00.398059Z","iopub.status.idle":"2024-10-18T02:33:15.090732Z","shell.execute_reply":"2024-10-18T02:33:15.089677Z","shell.execute_reply.started":"2024-10-18T02:33:00.398929Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting autocorrect\n","  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: autocorrect\n","  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=ce02437a09f84d5e59d3555ad2a6104c4dd0db6020ff03c1d5a34bb9b155dd1d\n","  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n","Successfully built autocorrect\n","Installing collected packages: autocorrect\n","Successfully installed autocorrect-2.6.1\n"]}],"source":["!pip install autocorrect"]},{"cell_type":"markdown","metadata":{},"source":["# Load Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T02:32:25.469947Z","iopub.status.busy":"2024-10-18T02:32:25.469540Z","iopub.status.idle":"2024-10-18T02:32:29.876038Z","shell.execute_reply":"2024-10-18T02:32:29.875104Z","shell.execute_reply.started":"2024-10-18T02:32:25.469909Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8f6e8e0b519421d93d388b0e61bc5e7","version_major":2,"version_minor":0},"text/plain":["customerqueries_L.csv:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57300194dcb94004a5d46410fb995db9","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","ds = load_dataset(\"coeuslearning/customerqueries\")"]},{"cell_type":"markdown","metadata":{},"source":["# Put Dataset In Simple Structure"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T02:32:37.458832Z","iopub.status.busy":"2024-10-18T02:32:37.457643Z","iopub.status.idle":"2024-10-18T02:32:39.003928Z","shell.execute_reply":"2024-10-18T02:32:39.002907Z","shell.execute_reply.started":"2024-10-18T02:32:37.458787Z"},"trusted":true},"outputs":[],"source":["dataset=[]\n","dataset_length=ds[\"train\"].num_rows\n","for index in range(dataset_length):\n","    dataset.append({\"query\":ds[\"train\"][index][\"Query\"],\"answer\":ds[\"train\"][index][\"Answer\"]})"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T02:32:42.772501Z","iopub.status.busy":"2024-10-18T02:32:42.771600Z","iopub.status.idle":"2024-10-18T02:32:42.779275Z","shell.execute_reply":"2024-10-18T02:32:42.778277Z","shell.execute_reply.started":"2024-10-18T02:32:42.772457Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'query': 'How to connect the Printer to a computer?',\n","  'answer': 'To connect the Printer to a computer, use the provided USB cable and follow the instructions in the user manual.'},\n"," {'query': 'What are the dimensions of the Scanner?',\n","  'answer': 'The dimensions of the Scanner are 10 x 8 x 5 inches.'},\n"," {'query': 'Can the Laptop be used with both Windows and Mac operating systems?',\n","  'answer': 'Yes, the Laptop is compatible with both Windows and Mac operating systems.'}]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset[:3]"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T02:33:25.858292Z","iopub.status.busy":"2024-10-18T02:33:25.857830Z","iopub.status.idle":"2024-10-18T02:33:28.440528Z","shell.execute_reply":"2024-10-18T02:33:28.439376Z","shell.execute_reply.started":"2024-10-18T02:33:25.858254Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n","   creating: /usr/share/nltk_data/corpora/wordnet/\n","  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n","  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n"]}],"source":["import re\n","from autocorrect import Speller\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk import word_tokenize\n","import string\n","nltk.download(\"stopwords\")\n","nltk.download('punkt')\n","nltk.download(\"wordnet\")\n","\n","! unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n","\n","\n","def remove_html_tags(text):\n","    html_pattern = r'<.*?>'\n","    without_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n","    return without_html\n","\n","def convert_to_lower(text):\n","    return text.lower()\n","\n","def remove_urls(text):\n","    url_pattern = r'https?://\\S+|www\\.\\S+'\n","    without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n","    return without_urls\n","\n","def spell_checker(text):\n","    spellChecker = Speller(lang=\"en\")\n","    correct_words = []\n","    for word in nltk.word_tokenize(text):\n","        correct_word = spellChecker(word)\n","        correct_words.append(correct_word)\n","    correct_spell_text = \" \".join(correct_words)\n","    return correct_spell_text\n","\n","\n","def remove_punctuation(text):\n","    return text.translate(str.maketrans('', '', string.punctuation))\n","\n","\n","def remove_stopwords(text):\n","    removed = []\n","    stop_words = list(stopwords.words(\"english\"))\n","    tokens = word_tokenize(text)\n","    for i in range(len(tokens)):\n","        if tokens[i] not in stop_words:\n","            removed.append(tokens[i])\n","    return \" \".join(removed)\n","\n","\n","def lemmatizing(text):\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = word_tokenize(text)\n","    for i in range(len(tokens)):\n","        lemma_word = lemmatizer.lemmatize(tokens[i])\n","        tokens[i] = lemma_word\n","    return \" \".join(tokens)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T02:33:35.786822Z","iopub.status.busy":"2024-10-18T02:33:35.785116Z","iopub.status.idle":"2024-10-18T02:33:35.792244Z","shell.execute_reply":"2024-10-18T02:33:35.791323Z","shell.execute_reply.started":"2024-10-18T02:33:35.786774Z"},"trusted":true},"outputs":[],"source":["def clean(text):\n","    cleaned_text=convert_to_lower(text)\n","    cleaned_text=remove_html_tags(cleaned_text)\n","    cleaned_text=remove_urls(cleaned_text)\n","    cleaned_text=remove_punctuation(cleaned_text)\n","    cleaned_text=remove_stopwords(cleaned_text)\n","    cleaned_text=lemmatizing(cleaned_text)\n","    cleaned_text= spell_checker(cleaned_text)\n","    return  cleaned_text\n","    \n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Apply preprocessing on dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T02:33:40.773724Z","iopub.status.busy":"2024-10-18T02:33:40.772832Z","iopub.status.idle":"2024-10-18T03:14:40.352042Z","shell.execute_reply":"2024-10-18T03:14:40.350666Z","shell.execute_reply.started":"2024-10-18T02:33:40.773681Z"},"trusted":true},"outputs":[],"source":["cleaned_dataset=[]\n","for data in dataset:\n","    cleaned_dataset.append({\"query\":clean(data[\"query\"]),\"answer\":clean(data[\"answer\"])})\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T03:14:56.536848Z","iopub.status.busy":"2024-10-18T03:14:56.536440Z","iopub.status.idle":"2024-10-18T03:15:01.441618Z","shell.execute_reply":"2024-10-18T03:15:01.440641Z","shell.execute_reply.started":"2024-10-18T03:14:56.536807Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"135e169e63654883b4b47118509c9eae","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cea89ccf88c644be9934744e45df1d69","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d977dc99c162492099f6fa2cd7b18987","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f73b11492fa47fb8f0863e29d89649d","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fbbf7714e2c443ca71672fd422736f6","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2062be45eb7845ff959c0b161f590c86","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n","from torch.utils.data import DataLoader, Dataset\n","\n","class QADataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=512):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        query = self.data[idx]['query']\n","        answer = self.data[idx]['answer']\n","        encoding = self.tokenizer(\n","            query,\n","            answer,\n","            truncation=True,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        )\n","        \n","        input_ids = encoding['input_ids'].flatten()\n","        labels = encoding['input_ids'].flatten()\n","        attention_mask = encoding['attention_mask'].flatten()  \n","\n","        return {\n","            'input_ids': input_ids,\n","            'labels': labels,\n","            'attention_mask': attention_mask \n","        }\n","\n","\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","qa_dataset = QADataset(cleaned_dataset, tokenizer)\n","train_loader = DataLoader(qa_dataset, batch_size=2, shuffle=True)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T03:15:10.462470Z","iopub.status.busy":"2024-10-18T03:15:10.461392Z","iopub.status.idle":"2024-10-18T03:15:10.466528Z","shell.execute_reply":"2024-10-18T03:15:10.465562Z","shell.execute_reply.started":"2024-10-18T03:15:10.462427Z"},"trusted":true},"outputs":[],"source":["tokenizer.pad_token=tokenizer.eos_token"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T03:15:20.622437Z","iopub.status.busy":"2024-10-18T03:15:20.621652Z","iopub.status.idle":"2024-10-18T03:48:24.427395Z","shell.execute_reply":"2024-10-18T03:48:24.426348Z","shell.execute_reply.started":"2024-10-18T03:15:20.622397Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d547ac847a94e1a9ebdccbd2bdd3e65","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd124f0df12147f693f290cd12482222","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2, Loss: 0.00038444908568635583\n","Epoch 2/2, Loss: 0.0003422353183850646\n"]},{"data":{"text/plain":["('./fine_tuned_gpt2_qa/tokenizer_config.json',\n"," './fine_tuned_gpt2_qa/special_tokens_map.json',\n"," './fine_tuned_gpt2_qa/vocab.json',\n"," './fine_tuned_gpt2_qa/merges.txt',\n"," './fine_tuned_gpt2_qa/added_tokens.json')"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import GPT2LMHeadModel, AdamW\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","model.train()\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","epochs = 2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","for epoch in range(epochs):\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        labels = batch['labels'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)  \n","        optimizer.zero_grad()\n","        outputs = model(input_ids, labels=labels, attention_mask=attention_mask)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n","\n","model.save_pretrained(\"./fine_tuned_gpt2_qa\")\n","tokenizer.save_pretrained(\"./fine_tuned_gpt2_qa\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T03:49:08.716959Z","iopub.status.busy":"2024-10-18T03:49:08.716148Z","iopub.status.idle":"2024-10-18T03:49:09.770177Z","shell.execute_reply":"2024-10-18T03:49:09.769326Z","shell.execute_reply.started":"2024-10-18T03:49:08.716915Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Question: I try all solutions but printer does not work\n","Answer: try solution printer workbenchmarks usb 30yes monitor support resolution printer 110 dpi\n"]}],"source":["import re\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","fine_tuned_model = GPT2LMHeadModel.from_pretrained('/kaggle/working/fine_tuned_gpt2_qa')\n","tokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/fine_tuned_gpt2_qa')\n","fine_tuned_model.eval()\n","def generate_answer(query):\n","    query = clean(query)\n","    inputs = tokenizer.encode(query, return_tensors='pt')\n","    attention_mask = (inputs != tokenizer.pad_token_id).long() \n","    outputs = fine_tuned_model.generate(inputs, attention_mask=attention_mask, max_length=100, num_return_sequences=1)\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","\n","question = \"I try all solutions but printer does not work\"\n","answer = generate_answer(question)\n","modified_answer = re.sub(question, \"\", answer)\n","answer=modified_answer.strip()\n","print(f\"Question: {question}\")\n","print(f\"Answer: {answer}\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-18T03:49:48.317065Z","iopub.status.busy":"2024-10-18T03:49:48.316045Z","iopub.status.idle":"2024-10-18T03:50:16.124814Z","shell.execute_reply":"2024-10-18T03:50:16.123829Z","shell.execute_reply.started":"2024-10-18T03:49:48.317007Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ChatbotCheckpoint.zip created successfully!\n"]}],"source":["import zipfile\n","import os\n","folder_to_compress = '/kaggle/working/fine_tuned_gpt2_qa'  \n","zip_file_name = 'ChatbotCheckpoint.zip'  \n","with zipfile.ZipFile(zip_file_name, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n","    for root, dirs, files in os.walk(folder_to_compress):\n","        for file in files:\n","            file_path = os.path.join(root, file)\n","            zip_file.write(file_path, os.path.relpath(file_path, folder_to_compress))\n","\n","print(f'{zip_file_name} created successfully!')\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
